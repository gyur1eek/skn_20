{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download('aashita/nyt-comments')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "article_lists = glob(path+'/*.*',recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(article_lists[0]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "##### LSTM\n",
    "- 입력 : \"나는 파이썬을 좋아합니다. 따라서 나는 ___ 을 잘합니다.\"\n",
    "- 일반신경망 : 공부  ( 파이썬 정보가 희석... 잊어)\n",
    "- LSTM : 프로그래밍(오래된 정보도 기억)\n",
    "\n",
    "- 핵심 키워드\n",
    "    - 장기기억 : 중요한 정보는 오래기억\n",
    "    - 단기기억 : 불필요한 정보는 버림\n",
    "    - 순서이해 : 시간순서 이해\n",
    "\n",
    "##### 3개의 Gate를 통해 정보의 흐름을 제어\n",
    " - LSTM 셀 (한 시점 t)\n",
    "    - 입력 : $x_t$ (현재데이터)\n",
    "    - 이전 은닉상태 : $h_{t-1}$\n",
    "    - 이전 셀 상태 : $c_{t-1}$\n",
    "\n",
    "    --> forget gate --> input date   --> output gate\n",
    "        잊을데이터        추가할 데이터     출력할 데이터\n",
    "    \n",
    "    출력 $h_t$ ,  $c_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Forget Gate(잊음 관문)\n",
    "\n",
    "\n",
    "$f_t$ = $s(w_f . [h_{t-1}, x_t ] + b_f )$\n",
    "\n",
    "s : sigmoid함수(0~1)\n",
    "\n",
    "이전 셀상태 : [1.5, -0.3, 2.1]\n",
    "현재입력 : '새로운 문장 입력'\n",
    "\n",
    "$f_t$ = [0.1,0.05,0.9]\n",
    "\n",
    "결과 : [ 1.5*0.1, -0.3*0.05, 2.1*0.9  ]  =  [0.15, -0.015, 1.89]\n",
    "\n",
    "첫 2개는 버리고 3번째는 유지\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "##### input gate : 입력\n",
    "- 첫 번째는 70%받고 두번째는 30% 받음\n",
    "\n",
    "##### Cell State 업데이트\n",
    " - 이전기억에서 필요한 것만 유지하고 새로운 정보에서 필요한 것만 유지\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "input 게이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "시점 t에서:\n",
    "- $x_t$      : 현재 입력 데이터 (벡터)\n",
    "- $h_{t-1}$  : 이전 시점의 은닉 상태 (벡터)\n",
    "- $C_{t-1}$  : 이전 시점의 셀 상태 (벡터) ← 장기 기억!\n",
    "- $W_*$, $U_*$ : 가중치 행렬\n",
    "- $b_*$      : 편향 벡터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "- $x_t$ = [0.2, -0.5, 0.8]      (3개 입력 피처)\n",
    "- $h_{t-1}$ = [0.1, 0.3, -0.2, 0.5]  (4개 은닉)\n",
    "- $C_{t-1}$ = [0.4, -0.1, 0.6, 0.2]  (4개 셀 상태)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Forget Gate ( $f_t$ ) - 어제 기억을 얼마나 유지할까\n",
    "- $f_t$ = $σ( W_f · [h_{t-1}, x_t] + b_f )$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "1단계: $h_{t-1}$ 과 $x_t$ 를 연결 (concatenate)\n",
    "\n",
    "   $[ h_{t-1}, x_t]$ = [0.1,    0.3,    -0.2,    0.5,    0.2,    -0.5,    0.8]\n",
    "\n",
    "               $h_{t-1}$    $x_t$\n",
    "\n",
    "                        4개              3개\n",
    "\n",
    "                               ↓\n",
    "\n",
    "                           총 7개 벡터\n",
    "\n",
    "2단계: 가중치 행렬 곱하기\n",
    "   $W_f · [h_{t-1}, x_t]$\n",
    "   \n",
    "   $W_f$ 는 크기: (4, 7) 행렬\n",
    "   (왜 (4, 7)? → 은닉 크기 4개, 입력 7개)\n",
    "   \n",
    "   결과: 4개의 값\n",
    "\n",
    "3단계: 편향 더하기\n",
    "   + $b_f$  (크기: 4)\n",
    "   \n",
    "   결과: 4개의 값\n",
    "\n",
    "4단계: Sigmoid 함수 적용\n",
    "   $σ(x)$ = $\\frac{1}{1 + e^{-x}}$ \n",
    "   \n",
    "   이 함수는 모든 값을 0~1 사이로 변환!\n",
    "   \n",
    "   $f_t = [0.3, 0.8, 0.1, 0.9]$\n",
    "   \n",
    "   의미:\n",
    "   - 첫 번째 셀 상태: 30% 유지 (70% 잊음)\n",
    "   - 두 번째 셀 상태: 80% 유지 (20% 잊음)\n",
    "   - 세 번째 셀 상태: 10% 유지 (90% 잊음)\n",
    "   - 네 번째 셀 상태: 90% 유지 (10% 잊음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_headline = []\n",
    "articles = [path for path in article_lists if \"Articles\" in path]\n",
    "# headline 정보만 추출 all_headline에 추가\n",
    "# 전처리 : 소문자로 변경하고 특수문자 제거\n",
    "import string\n",
    "\n",
    "for a in articles:\n",
    "    df = pd.read_csv(a)\n",
    "    df.headline.values\n",
    "    all_headline.extend( df.headline.values )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(article_lists[0])\n",
    "cleaned_sentence = [doc.lower() for doc in df.headline.values if doc not in string.punctuation ]\n",
    "\n",
    "\n",
    "# 모든 문장의 단어를 추출해 고유 번호 지정\n",
    "bow = {}\n",
    "for line in cleaned_sentence:\n",
    "    for w in line.split():\n",
    "        if w not in bow:\n",
    "            bow[w] = len(bow.keys())\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "[bow[w] for w in cleaned_sentence[5].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "# kaggle data download\n",
    "import kagglehub\n",
    "path = kagglehub.dataset_download('aashita/nyt-comments')\n",
    "\n",
    "# csv파일이 있는 경로 path\n",
    "csv_lists = glob(path+'/*.*')\n",
    "\n",
    "class TextGeneration(Dataset):\n",
    "    def clean_text(self, txt):\n",
    "        # 모든 단어를 소문자로 바꾸고 특수문자를 제거\n",
    "        txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "        return txt\n",
    "    def __init__(self,csv_lists):\n",
    "        all_headlines = []\n",
    "\n",
    "        # 모든 헤드라인의 텍스트를 불러옴\n",
    "        for filename in csv_lists:\n",
    "            if 'Articles' in filename:\n",
    "                article_df = pd.read_csv(filename)\n",
    "\n",
    "                # 데이터셋의 headline의 값을 all_headlines에 추가\n",
    "                all_headlines.extend(list(article_df.headline.values))\n",
    "                break\n",
    "\n",
    "        # headline 중 unknown 값은 제거\n",
    "        all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
    "        \n",
    "        # 구두점 제거 및 전처리가 된 문장들을 리스트로 반환\n",
    "        self.corpus = [self.clean_text(x) for x in all_headlines]\n",
    "        self.BOW = {}\n",
    "\n",
    "        # 모든 문장의 단어를 추출해 고유번호 지정\n",
    "        for line in self.corpus:\n",
    "            for word in line.split():\n",
    "                if word not in self.BOW.keys():\n",
    "                    self.BOW[word] = len(self.BOW.keys())\n",
    "\n",
    "        # 모델의 입력으로 사용할 데이터\n",
    "        self.data = self.generate_sequence(self.corpus)\n",
    "    def generate_sequence(self, txt):\n",
    "        seq = []\n",
    "\n",
    "        for line in txt:\n",
    "            line = line.split()\n",
    "            line_bow = [self.BOW[word] for word in line]\n",
    "\n",
    "            # 단어 2개를 입력으로, 그다음 단어를 정답으로\n",
    "            data = [([line_bow[i], line_bow[i+1]], line_bow[i+2]) \n",
    "            for i in range(len(line_bow)-2)]\n",
    "            \n",
    "            seq.extend(data)\n",
    "\n",
    "        return seq\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, i):\n",
    "        data = np.array(self.data[i][0])  # 입력 데이터\n",
    "        label = np.array(self.data[i][1]).astype(np.float32)  # 출력 데이터\n",
    "\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 모델 정의\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_embeddings): #num_embeddings 전체 단얼의 개수(어휘사전 크기)\n",
    "        super(LSTM, self).__init__()  \n",
    "        #embedding은 신경망이 이해할 수 있도록 벡터로 변경 \n",
    "        self.embed =nn.Embedding( num_embeddings=num_embeddings, embedding_dim=16)\n",
    "        # LSTM을 5개층 (배치, 시퀀스, 피처) 16~512\n",
    "        self.lstm = nn.LSTM(input_size=16, hidden_size=64, num_layers=5, batch_first=True)\n",
    "        # 분류를 위한 fc층\n",
    "        self.fc1 = nn.Linear( 128 , num_embeddings)\n",
    "        self.fc2 = nn.Linear( num_embeddings, num_embeddings)\n",
    "        self.relu =nn.ReLU()\n",
    "    def forward(self, X): # 입력 (배치크기, sequence_length) 배치 32, sequence_lengh 2\n",
    "        X = self.embed(X) # 출력(배치크기, sequence_length, 16 ) 32 2 16\n",
    "\n",
    "        #lstm 모델 예측값\n",
    "        X,_ = self.lstm(X) #출력 (batch sq_len, 64)\n",
    "        X = torch.reshape(X, (X.shape[0],-1))\n",
    "        X = self.relu(self.fc1(X))\n",
    "        out = self.fc2(X)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델....\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim.adam import Adam\n",
    "from tqdm import tqdm\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dataset = TextGeneration(csv_lists)\n",
    "model = LSTM(num_embeddings=len(dataset.BOW)).to(device)\n",
    "loader = DataLoader(dataset, batch_size=32)\n",
    "optim = Adam(model.parameters(), lr=1e-3)\n",
    "for epoch in range(200):\n",
    "    loop =tqdm(loader)\n",
    "    for data, label in loop:\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred = model(torch.tensor(data,dtype =torch.long))\n",
    "        loss = nn.CrossEntropyLoss()(pred, torch.tensor(label,dtype=torch.long))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        loop.set_description(f'epoch:{epoch+1} loss : {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장을 예측\n",
    "# 입력문 창을 텐서로 변경 임베딩 벡터 Bow을 이용해서\n",
    "sample = \"i love\"\n",
    "with torch.no_grad():\n",
    "    words = torch.tensor(\n",
    "        [dataset.BOW[w] for w in sample.split()],dtype=torch.long\n",
    "    ).to(device).unsqueeze(0)\n",
    "    #(2,)--> (batch, sq_len) (1,2)\n",
    "\n",
    "output = model(words)\n",
    "# 출력은 단어의 개수 만큼 len(BOW) (batch, len(BOW))\n",
    "# 확률이 가장 높은 단어 찾기\n",
    "predicted_index = torch.argmax(output,dim=1).item()\n",
    "\n",
    "# 단어사전 BOW에서 인덱스에서 해당하는 단어를 찾기\n",
    "# 역 dict를 만들어서 찾기\n",
    "reverse_bow = {value: key for key, value in dataset.BOW.items()} # key.value나옴\n",
    "predicted_word = reverse_bow[predicted_index]\n",
    "print(predicted_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
